---
Title: Artificial Intelligence
Template: LeafPage
GridImage: media/ai-thumbnail.png
---



# Artificial Intelligence




Artifcial Intelligence (AI) broadly incorporates simulated intelligence in machines. An
exact definition is not straightforward to give, as it depends on whether we want a system to *act* 
intelligently or *think* intelligently, and on whether we want the system to do this like a *human* or
 *rationally*.

Unlike many of the other topics covered by Ethics in Maths, the ethics of AI is widely talked about and discussed, although generally this isn't aimed at mathematicians. ["Want Less-Biased Decisions? Use Algorithms."](https://hbr.org/2018/07/want-less-biased-decisions-use-algorithms), and article in the *Harvard Business Review*, claims that "Algorithms are less biased and more accurate than the humans
they are replacing." This is a very dangerous claim to make. Algorithms can make mistakes, and we are definitely not at the stage where they are ready to be replacing any human decision-makers.  ["The Real Threat of Artificial Intelligence"](https://www.nytimes.com/2017/06/24/opinion/sunday/artificial-intelligence-economic-inequality.html) by the *New York Times*
 says that AI is "poised to bring about a wide-scale decimation of jobs." While this is an issue worth considering, it suggests a naive view on the current state of intelligence, especially since a lot of AI is being designed to assist humans in their jobs, not take their jobs away. The book *Superintelligence: Paths, Dangers, Strategies*, by Oxford philosopher Nick Bostrom, talks about the "intelligence explosion" that will occur if (or rather when) machines which are more intelligent than humans begin to design machines of their own. This book contains many important philosophical points, but ultimately is purely speculative and difficult to take seriously. The ethics in AI is a serious issue which needs to be discussed as such.

 
 <div id=grid>
 
 <a href="https://cueimps.soc.srcf.net/course/course/AI/HBRarticle">
 <img src="http://cueimps.soc.srcf.net/course/media/Lara/HBRimage.png"/>
 </a>
 
  <a href="https://cueimps.soc.srcf.net/course/course/AI/NYtimesReview">
 <img src="http://cueimps.soc.srcf.net/course/media/Lara/NYTimageSmall.jpg"/>
 </a>
 
  <a href="https://cueimps.soc.srcf.net/course/course/AI/superintelligence">
 <img src="http://cueimps.soc.srcf.net/course/media/Lara/BostromImageSmall.jpg"/>
 </a>
 
 </div>
 
 
 
 

One of the major issues which a lot of the material on ethics in AI aimed at the general public seems to neglect is the concept that AI is still susceptible to human error. In March 2018, an autonomous vehicle killed a pedestrian for the first time. Many issues led to this catastrophic event, like the car's emergency breaking system being disabled, thus relying on the (distracted) car 'driver' to brake should it be required in an emergency. In decision-making algorithms, bias is a huge issue. 

[![](http://cueimps.soc.srcf.net/course/media/Lara/UBERimageSmall.png)](https://cueimps.soc.srcf.net/course/course/AI/autocars)

In the case of deep neural networks (a branch of machine learning), it is currently not possible to see why an algorithm made a specific decision. These deep neural networks make decisions based on a set of training data, where it is give what the outcome should be. It 'learns' what values certain parameters should have based on this data, and subsequently uses these parameters to make decisions based on other input data. Any bias in the training data leads to bias in the algorithms decisions. A group at MIT created an 'AI-powered psychopath' in 2018 to prove this point. This makes it dangerous to use these algorithms in, for example, prison sentencing, where historic bias could lead to bias in the training data leading to bias in the decision made by the algorithm.

Those creating AI algorithms not only need to be careful about the intended uses for the technology, but also the potential uses. In February 2018, a paper entitled "Deep neural networks can detect sexual orientation from faces" was published in the Journal of Personality and Social Psychology. The people who wrote the paper used widely available deep neural networks to do the study - they didn't need very much understanding about how machine learning works in order to use it in this manner. The technical people creating the algorithms need to have an awareness about the potential uses of the tools they create so that they can hopefully make a step towards preventing immoral usage. 


 <div id=grid>
 
 <a href="https://cueimps.soc.srcf.net/course/course/AI/Bias">
 <img src="http://cueimps.soc.srcf.net/course/media/Lara/BiasImage.png"/>
 </a>
 
  <a href="https://cueimps.soc.srcf.net/course/course/AI/kosinski">
 <img src="http://cueimps.soc.srcf.net/course/media/Lara/kosinskiImage.jpg"/>
 </a>
 

 </div>





