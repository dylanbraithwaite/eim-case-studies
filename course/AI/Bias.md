---
Title: Bias
Template: ListSubPages
---



# Bias
On April 1st 2018, a group at MIT created an 'AI-powered psychopath' named Norman [1]. Norman is a machine learning algorithm trained for image captioning. The idea is that Norman is shown an image, and it will give a caption to accompany the image. For a traditional image captioning algorithm, the output would ideally describe what the image contains. However, Norman was trained on "image captions from an infamous subreddit (the name is redacted due to its graphic content) that is dedicated to document and observe the disturbing reality of death." Then, Norman's responses to being shown randomly generated images of inkblots were compared with a standard image captioning neural network. Unsurprisingly, despite the variety of responses of the standard image captioning neural network output, Norman saw people dying in every image. While this is an extreme example, this does demonstrate that the data that machine learning algorithms 'learn' from has a big affect on the output. Therefore, if the data input into decision making algorithms are biased, the decisions that the algorithm makes will be biased. 

Points to add:
- data is not the only way an algorithm can be biased

- bias in an algorithm cannot be fixed by another algorithm

- how do you measure bias (link to fairness)

### References

[1] [Norman, by MIT Media Lab](http://norman-ai.mit.edu)
